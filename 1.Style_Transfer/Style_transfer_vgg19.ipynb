{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a714a22e",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb442d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e8a82",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe390750",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153f9b8",
   "metadata": {},
   "source": [
    "# Image to Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(img_path, img_size):\n",
    "    loader = transforms.Compose([\n",
    "     transforms.Resize(img_size), # Resize image size\n",
    "     transforms.ToTensor() # Make Tensor\n",
    "    ])\n",
    "    \n",
    "    img = PIL.Image.open(img_path)\n",
    "    \n",
    "    # Add dimension to img to make batch for model_input\n",
    "    img = loader(img).unsqueeze(0)\n",
    "    return img.to(device, torch.float) # on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929ec0b",
   "metadata": {},
   "source": [
    "# torch.Tensor -> img to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor):\n",
    "    # matplotlib is based on cpu\n",
    "    image = tensor.cpu().clone()\n",
    "    # Remove dimension for batch \n",
    "    image = image.squeeze(0)\n",
    "    # to PIL\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    \n",
    "    # Display\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b37c92",
   "metadata": {},
   "source": [
    "# Image Reconstruction\n",
    "\n",
    "* Optimizing image means updating to get lower total loss<br>\n",
    "* using MSE as Loss_func, make noise image to target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9642e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image path\n",
    "img_path = 'cat.jpg'\n",
    "target_image = image_loader(img_path, (512, 512))\n",
    "imshow(target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get same size noise image\n",
    "noise = torch.empty_like(target_image).uniform_(0, 1).to(device)\n",
    "imshow(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss() # Set Loss function\n",
    "iters = 100 # Set iteration numbers\n",
    "lr = 1e4\n",
    "\n",
    "print('[ Start ]')\n",
    "imshow(noise)\n",
    "\n",
    "for i in range(iters):\n",
    "    # set required_grad True to trace torch.Tensor\n",
    "    noise.requires_grad = True\n",
    "    \n",
    "    # Get gradient\n",
    "    output = loss(noise, target_image)\n",
    "    output.backward()\n",
    "    \n",
    "    # Update to make loss smaller\n",
    "    gradient = lr * noise.grad\n",
    "    \n",
    "    # clip noise_image [0, 1]\n",
    "    noise = torch.clamp(noise - gradient, min=0, max=1).detach_() # detach -> stop Tracking\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'[ Step: {i+1} ]')\n",
    "        print(f'Loss: {output}')\n",
    "        imshow(noise)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578d55a",
   "metadata": {},
   "source": [
    "# Get Image for Style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d14c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get content image, style image\n",
    "content_img = image_loader('content_img_3.jpg', (512, 640))\n",
    "style_img = image_loader('style_img_1.jpg', (512, 640))\n",
    "\n",
    "imshow(content_img)\n",
    "imshow(style_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b1eb5",
   "metadata": {},
   "source": [
    "# Load CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15718b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.clone().view(-1,1,1)\n",
    "        self.std = std.clone().view(-1,1,1)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa346151",
   "metadata": {},
   "source": [
    "# Style Reconstruction\n",
    "* Make noise image to be similar with content and style image\n",
    "* Get 5 style_layers and 1 content_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eca45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    # a: batch_size, b: feature_map_numbers, (c,d) -> feature map dimension\n",
    "    a, b, c, d = input.size()\n",
    "    \n",
    "    # i == feature_map_numbers, j == position\n",
    "    features = input.view(a * b, c * d)\n",
    "    \n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a*b*c*d)\n",
    "\n",
    "# Style loss\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "# 스타일 손실(style loss)을 계산하는 함수\n",
    "def get_style_losses(cnn, style_img, noise_image):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "    style_losses = []\n",
    "    \n",
    "    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        # 설정한 style layer까지의 결과를 이용해 style loss를 계산\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # 마지막 style loss 이후의 레이어는 사용하지 않도록\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "    return model, style_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6299bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_reconstruction(cnn, style_img, input_img, iters):\n",
    "    model, style_losses = get_style_losses(cnn, style_img, input_img)\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()], lr=0.1)\n",
    "    \n",
    "    print('[Start]')\n",
    "    imshow(input_img)\n",
    "    \n",
    "    run = [0]\n",
    "    while run[0] <= iters:\n",
    "        \n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            \n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            \n",
    "            style_score *= 1e6\n",
    "            style_score.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f'[step : {run[0]} / Style loss : {style_score.item()}]')\n",
    "                imshow(input_img)\n",
    "            return style_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    # clip image [0, 1]\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    \n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.empty_like(content_img).uniform_(0,1).to(device)\n",
    "imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe8a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Style Reconstruction\n",
    "output = style_reconstruction(cnn, style_img=style_img, input_img=input_img, iters=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d17a7e",
   "metadata": {},
   "source": [
    "# Content Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Content Loss\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, ):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['conv_4']\n",
    "\n",
    "# 콘텐츠 손실(content loss)을 계산하는 함수\n",
    "def get_content_losses(cnn, content_img, noise_image):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "    content_losses = []\n",
    "    \n",
    "    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        # 설정한 content layer까지의 결과를 이용해 content loss를 계산\n",
    "        if name in content_layers:\n",
    "            target_feature = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target_feature)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "    # 마지막 content loss 이후의 레이어는 사용하지 않도록\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "    return model, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb45e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_reconstruction(cnn, content_img, input_img, iters):\n",
    "    model, content_losses = get_content_losses(cnn, content_img, input_img)\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()], lr=0.1)\n",
    "\n",
    "    print(\"[ Start ]\")\n",
    "    imshow(input_img)\n",
    "\n",
    "    # 하나의 값만 이용하기 위해 배열 형태로 사용\n",
    "    run = [0]\n",
    "    while run[0] <= iters:\n",
    "\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            content_score = 0\n",
    "\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            content_score.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"[ Step: {run[0]} / Content loss: {content_score.item()}]\")\n",
    "                imshow(input_img)\n",
    "            \n",
    "            return content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # 결과적으로 이미지의 각 픽셀의 값이 [0, 1] 사이의 값이 되도록 자르기\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3389cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.empty_like(content_img).uniform_(0,1).to(device)\n",
    "imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de746b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Content Reconstruction\n",
    "output = content_reconstruction(cnn, content_img=content_img, input_img=input_img, iters=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3155d",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "* use Content Loss and Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a28c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_3', 'conv_5', 'conv_7', 'conv_9']\n",
    "\n",
    "# Style Transfer 손실(loss)을 계산하는 함수\n",
    "def get_losses(cnn, content_img, style_img, noise_image):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        # 설정한 content layer까지의 결과를 이용해 content loss를 계산\n",
    "        if name in content_layers:\n",
    "            target_feature = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target_feature)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        # 설정한 style layer까지의 결과를 이용해 style loss를 계산\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # 마지막 loss 이후의 레이어는 사용하지 않도록\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "    return model, content_losses, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(cnn, content_img, style_img, input_img, iters, lr, patience):\n",
    "    model, content_losses, style_losses = get_losses(cnn, content_img, style_img, input_img)\n",
    "    \n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()], lr=lr)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience)\n",
    "    print(\"[ Start ]\")\n",
    "    imshow(input_img)\n",
    "\n",
    "    # 하나의 값만 이용하기 위해 배열 형태로 사용\n",
    "    run = [0]\n",
    "    while run[0] <= iters:\n",
    "\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            content_score = 0\n",
    "            style_score = 0\n",
    "\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "\n",
    "            style_score *= 1e5\n",
    "            loss = content_score + style_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 100 == 0:\n",
    "                print(f\"[ Step: {run[0]} / Content loss: {content_score.item()} / Style loss: {style_score.item()}]\")\n",
    "                imshow(input_img)\n",
    "                save_image(input_img.cpu().detach()[0], f'{run[0]}.png') \n",
    "            \n",
    "            return content_score + style_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        scheduler.step(closure())\n",
    "\n",
    "    # 결과적으로 이미지의 각 픽셀의 값이 [0, 1] 사이의 값이 되도록 자르기\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e335f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get content image, style image\n",
    "content_img = image_loader('content_img_3.jpg', (512, 640))\n",
    "style_img = image_loader('style_img_1.jpg', (512, 640))\n",
    "input_img = torch.empty_like(content_img).uniform_(0,1).to(device)\n",
    "\n",
    "imshow(content_img)\n",
    "imshow(style_img)\n",
    "imshow(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3cd0f6",
   "metadata": {},
   "source": [
    "# Run style-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b51991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = style_transfer(cnn, content_img=content_img, style_img=style_img, input_img=input_img, iters=6000, lr=1, patience=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59fe1d",
   "metadata": {},
   "source": [
    "# Another samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b34c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get content image, style image\n",
    "content_img = image_loader('content_img_9.jpg', (650, 1000))\n",
    "style_img = image_loader('style_img_7.png', (650, 1000))\n",
    "input_img = torch.empty_like(content_img).uniform_(0,1).to(device)\n",
    "\n",
    "imshow(content_img)\n",
    "imshow(style_img)\n",
    "imshow(input_img)\n",
    "\n",
    "output = style_transfer(cnn, content_img=content_img, style_img=style_img, input_img=input_img, iters=6000, lr=1, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56977ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_img = image_loader('content_img_8.jpg', (650, 1000))\n",
    "style_img = image_loader('style_img_5.jpg', (650, 1000))\n",
    "input_img = torch.empty_like(content_img).uniform_(0,1).to(device)\n",
    "\n",
    "imshow(content_img)\n",
    "imshow(style_img)\n",
    "imshow(input_img)\n",
    "\n",
    "output = style_transfer(cnn, content_img=content_img, style_img=style_img, input_img=input_img, lr=1, iters=10000, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6803a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
